# -*- coding: utf-8 -*-
"""
Created on Thu Apr  5 10:09:38 2025
@author: Administrator
"""
# Bangash PhD Scholar

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report

from tensorflow.keras.utils import Sequence
from tensorflow.keras.applications import InceptionResNetV2
from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Concatenate, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import load_model

# === Step 1: Updated Paths ===
image_dirs = [
    " /path",
    " /path"
]
metadata_path = "/path"  # Updated metadata path
image_size = (224, 224)

# === Step 2: Load Metadata ===
metadata = pd.read_csv(metadata_path)

# Ensure columns are clean (strip spaces)
metadata.columns = metadata.columns.str.strip()

# Print the columns to check for 'sex' and 'localization'
print(metadata.columns)

# === Step 3: Handle Missing Columns (sex_encoded, localization_encoded) ===
# Encode 'sex' column if exists
if 'sex' in metadata.columns:
    label_encoder = LabelEncoder()
    metadata['sex_encoded'] = label_encoder.fit_transform(metadata['sex'])

# Encode 'localization' column if exists
if 'localization' in metadata.columns:
    label_encoder = LabelEncoder()
    metadata['localization_encoded'] = label_encoder.fit_transform(metadata['localization'])

# === Step 4: Encode Labels ===
metadata['filename'] = metadata['image_id'] + ".jpg"
label_encoder = LabelEncoder()
metadata['label_enc'] = label_encoder.fit_transform(metadata['dx'])

# === Step 5: Define Metadata Features ===
metadata_features = ['age', 'sex_encoded', 'localization_encoded']  # replace with your column names
X_meta = metadata[metadata_features].values
scaler = StandardScaler()
X_meta = scaler.fit_transform(X_meta)

# === Step 6: Load Image Paths ===
image_path_dict = {}
for folder in image_dirs:
    for fname in os.listdir(folder):
        if fname.endswith('.jpg'):
            image_path_dict[fname] = os.path.join(folder, fname)

metadata['image_path'] = metadata['filename'].map(image_path_dict)

# === Step 7: Train-Test Split ===
X_img_train, X_img_val, X_meta_train, X_meta_val, y_train, y_val = train_test_split(
    metadata['image_path'], X_meta, metadata['label_enc'], test_size=0.2, random_state=42, stratify=metadata['label_enc']
)

# === Step 8: Custom Data Generator ===
class MultiModalGenerator(Sequence):
    def __init__(self, image_paths, metadata, labels, batch_size=32, image_size=(224, 224), num_classes=7):
        self.image_paths = list(image_paths)
        self.metadata = metadata
        self.labels = labels
        self.batch_size = batch_size
        self.image_size = image_size
        self.num_classes = num_classes

    def __len__(self):
        return int(np.ceil(len(self.image_paths) / self.batch_size))

    def __getitem__(self, idx):
        batch_paths = self.image_paths[idx * self.batch_size:(idx + 1) * self.batch_size]
        batch_metadata = self.metadata[idx * self.batch_size:(idx + 1) * self.batch_size]
        batch_labels = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]

        batch_images = np.zeros((len(batch_paths), *self.image_size, 3), dtype=np.float32)

        for i, path in enumerate(batch_paths):
            img = load_img(path, target_size=self.image_size)
            img_array = img_to_array(img) / 255.0
            batch_images[i] = img_array

        # Return as tuple (features, labels)
        return (batch_images, batch_metadata), np.array(batch_labels)

# === Step 9: Create Generators ===
train_gen = MultiModalGenerator(X_img_train, X_meta_train, y_train)
val_gen = MultiModalGenerator(X_img_val, X_meta_val, y_val)

# === Step 10: Build Multi-Modal Model ===
# Image branch
image_input = Input(shape=(224, 224, 3))
base_model = InceptionResNetV2(include_top=False, weights='imagenet', input_tensor=image_input)
base_model.trainable = False
x = GlobalAveragePooling2D()(base_model.output)

# Metadata branch
meta_input = Input(shape=(X_meta.shape[1],))
m = Dense(64, activation='relu')(meta_input)
m = Dropout(0.3)(m)

# Merge
combined = Concatenate()([x, m])
z = Dense(128, activation='relu')(combined)
z = Dropout(0.3)(z)
z = Dense(len(label_encoder.classes_), activation='softmax')(z)

model = Model(inputs=[image_input, meta_input], outputs=z)

model.compile(optimizer=Adam(learning_rate=1e-4),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# === Step 11: Train the Model ===
history = model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=15
)

# === Step 12: Save the Model ===
model_save_path = "C:/Users/Administrator/Downloads/ExperimentsforImages/Deep_Learning/2025/saved_model.h5"
model.save(model_save_path)
print(f"Model saved to: {model_save_path}")

# === Step 13: Evaluation ===
# Get predictions
val_preds = model.predict(val_gen)
y_pred = np.argmax(val_preds, axis=1)

# Print Classification Report
print("\nClassification Report:\n")
print(classification_report(y_val, y_pred, target_names=label_encoder.classes_))

# Confusion Matrix
cm = confusion_matrix(y_val, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=label_encoder.classes_,
            yticklabels=label_encoder.classes_)
plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Confusion Matrix')
plt.show()

# === Step 14: Plot Accuracy and Loss ===
plt.figure(figsize=(12, 5))

# Accuracy Plot
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label="Train Accuracy")
plt.plot(history.history['val_accuracy'], label="Val Accuracy")
plt.legend(); plt.title("Accuracy")

# Loss Plot
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label="Train Loss")
plt.plot(history.history['val_loss'], label="Val Loss")
plt.legend(); plt.title("Loss")

plt.tight_layout(); plt.show()
